{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T12:21:52.844281Z",
     "start_time": "2019-08-23T12:19:21.759121Z"
    }
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import click\n",
    "import logging\n",
    "import pickle\n",
    "import cv2\n",
    "import resource\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# import supported models\n",
    "from tensorflow.keras.applications import ResNet50, MobileNet, MobileNetV2\n",
    "\n",
    "# import preprocessing functions for the models\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_input_mobile_net\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_input_mobile_net_v2\n",
    "\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def get_project_path():\n",
    "    try:\n",
    "        # running from a python file\"\n",
    "        project_path = Path(os.path.dirname(os.path.abspath(__file__)), os.pardir)\n",
    "        is_file = True\n",
    "        is_colab = False\n",
    "    except NameError as e:\n",
    "        # not running from a file (ie console or notebook)\n",
    "        project_path = Path(os.getcwd(), os.pardir)\n",
    "        is_file = False\n",
    "        try:\n",
    "            # running on google colab\n",
    "            from google.colab import drive\n",
    "            # connect to google drive\n",
    "            drive.mount('/content/gdrive')\n",
    "            project_path = Path(\"/content/gdrive/My Drive/Colab Notebooks/setdetection/\")\n",
    "            is_colab = True\n",
    "        except ModuleNotFoundError:\n",
    "            # not running on google colab\n",
    "            is_colab = False\n",
    "            output_path = project_path / \"output\"\n",
    "    return project_path, is_file, is_colab\n",
    "\n",
    "\n",
    "# get project path, and flag if script runs in a file\n",
    "project_path, is_file, is_colab = get_project_path()\n",
    "output_path = project_path / \"output\"\n",
    "results_csv_path = output_path / 'results.csv'\n",
    "\n",
    "# columns for results csv. This list is used to create the results dataframe\n",
    "result_columns = ['run_id', 'layer_id', 'layer_name', 'model', 'nr_images', 'normalize', 'train_score', 'val_score']\n",
    "\n",
    "# add project root to pythonpath\n",
    "sys.path.insert(0, str(project_path / \"src\"))\n",
    "\n",
    "# import custom packages\n",
    "from utils.identify import *    \n",
    "from utils.log import *\n",
    "from utils.click_utils import conditionally_decorate\n",
    "from set_cardgame.dataset import *\n",
    "\n",
    "# setup logging\n",
    "logger = setup_logger(level=logging.INFO)\n",
    "\n",
    "\n",
    "def get_or_create_results_file(results_csv_path, columns, sep=\";\"):\n",
    "    try:\n",
    "        df_results = pd.read_csv(results_csv_path, sep=sep)\n",
    "    except FileNotFoundError:\n",
    "        # create new results dataframe dictionary to store results\n",
    "        df_results = pd.DataFrame(columns=columns)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# decorate with click commands if the script runs from a file\n",
    "@conditionally_decorate(click.command(), is_file)\n",
    "@conditionally_decorate(click.option('--normalize', default=\"none\", help='how to normalize data, possible values: {\"standard\", \"minimax\"}.'),  is_file)\n",
    "@conditionally_decorate(click.option('--nr_images', default=81, help='Number of images to generate for training'), is_file)\n",
    "@conditionally_decorate(click.option('--model', default=\"resnet\", help='which model to use for embeddings: {\"resnet\", \"mobilenet\", \"mobilenet_v2\"}.'),  is_file)\n",
    "def train(normalize=\"none\", nr_images=81, model=\"resnet\"):\n",
    "    # make results repeatable\n",
    "    seed = 42\n",
    "    random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "    np.random.seed(seed)  # numpy pseudo-random generator\n",
    "    set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "    logger.info(f\"np.random.random(): {np.random.random()}\")\n",
    "\n",
    "    # create id for this run\n",
    "    run_id = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "\n",
    "    logger.info(\"======\")\n",
    "    logger.info(f\"starting run {run_id}\")\n",
    "    logger.info(f\"parameter: normalize: {normalize}\")\n",
    "    logger.info(f\"parameter: nr_images: {nr_images}\")\n",
    "    logger.info(f\"parameter: model: {model}\")\n",
    "    logger.info(f\"project_path: {project_path}\")\n",
    "    logger.info(f\"output_path: {output_path}\")\n",
    "    logger.info(f\"is_file: {is_file}\")\n",
    "    logger.info(f\"is_colab: {is_colab}\")\n",
    "\n",
    "    # initialize intermediate model\n",
    "    if model == \"resnet\":\n",
    "        model_instance = ResNet50(include_top=False, weights='imagenet', pooling=None, input_shape=(96, 128, 3))\n",
    "        preprocess_input_function = preprocess_input_resnet50\n",
    "    elif model == \"mobilenet\":\n",
    "        model_instance = MobileNet(include_top=False, weights='imagenet', pooling=None, input_shape=(96, 128, 3))\n",
    "        preprocess_input_function = preprocess_input_mobile_net\n",
    "    elif model == \"mobilenet_v2\":\n",
    "        model_instance = MobileNetV2(include_top=False, weights='imagenet', pooling=None, input_shape=(96, 128, 3))\n",
    "        preprocess_input_function = preprocess_input_mobile_net_v2\n",
    "    else:\n",
    "        logger.error(f\"unsupported model: {model}\")\n",
    "        return\n",
    "\n",
    "    # load training and validation data\n",
    "    X_train, y_train, X_val, y_val = load_dataset(nr_images=nr_images, output_path=None)\n",
    "\n",
    "    # apply preprocessing for the specified model\n",
    "    X_train = preprocess_input_function(X_train)\n",
    "    X_val = preprocess_input_function(X_val)\n",
    "\n",
    "    # nr samples\n",
    "    m_train = X_train.shape[0]\n",
    "    m_val = X_val.shape[0]\n",
    "    \n",
    "    logger.debug(\"X_train.shape:\", X_train.shape)\n",
    "    logger.debug(\"X_val.shape:\", X_val.shape)\n",
    "    logger.debug(\"m_train:\", m_train)\n",
    "    logger.debug(\"m_val:\", m_val)\n",
    "\n",
    "    # get results dataframe, or if it is not there create it\n",
    "    df_results = get_or_create_results_file(results_csv_path, result_columns)\n",
    "\n",
    "    # keep track of memory usage\n",
    "    mem_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "\n",
    "    # loop randomly through layers\n",
    "    nr_layers = len(model_instance.layers)\n",
    "    random_index = np.random.choice(range(1, nr_layers), size=nr_layers-1, replace=False)\n",
    "    for i, layer_id in enumerate(random_index):\n",
    "        logger.info(\"===\")\n",
    "        logger.info(f\"process layer: {i} - {layer_id}\")\n",
    "        # log memory usage for each iteration\n",
    "        mem_usage1 = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "        logger.info(f\"Memory usage: {mem_usage1}, diff: {mem_usage1-mem_usage}\")\n",
    "        mem_usage = mem_usage1\n",
    "\n",
    "        layer_name = model_instance.layers[layer_id].name\n",
    "\n",
    "        # check if there is a result for this layer_id, nr_images, normalize combination\n",
    "        cond_layer_id = (df_results['layer_id'] == layer_id)\n",
    "        cond_model = (df_results['model'] == model)\n",
    "        cond_nr_images = (df_results['nr_images'] == nr_images)\n",
    "        cond_normalize = (df_results['normalize'] == normalize)\n",
    "        df_previous_results = df_results[cond_layer_id & cond_model & cond_nr_images & cond_normalize]\n",
    "        logger.info(f\"{len(df_previous_results)} previous results for layer {layer_id}, nr_images {nr_images}, and normalize settings: {normalize}\")\n",
    "        if len(df_previous_results) > 0:\n",
    "            logger.info(f\"skip iteration - found previous results for layer {layer_id}, nr_images {nr_images}, and normalize settings: {normalize}.\")\n",
    "            logger.debug(f\"records found: {df_previous_results}\")\n",
    "            continue\n",
    "\n",
    "        # compile model, use specific layer as output\n",
    "        intermediate_model = Model(inputs=model_instance.input, outputs=model_instance.get_layer(layer_name).output)\n",
    "        intermediate_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=[categorical_accuracy])\n",
    "\n",
    "        # generate image embeddings\n",
    "        X_train_embeddings = intermediate_model.predict(X_train)\n",
    "        X_val_embeddings = intermediate_model.predict(X_val)\n",
    "\n",
    "        # reshape embeddings to 2D\n",
    "        X_train_embeddings = X_train_embeddings.reshape(m_train, -1)\n",
    "        X_val_embeddings = X_val_embeddings.reshape(m_val, -1)\n",
    "        \n",
    "        if normalize == \"standard\":\n",
    "            logger.info(\"normalize using standard scaling\")\n",
    "            scaler = StandardScaler()\n",
    "        elif normalize == \"minmax\":\n",
    "            logger.info(\"normalize using standard scaling\")\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            logger.info(\"no normalization\")\n",
    "            scaler = None\n",
    "\n",
    "        if scaler:\n",
    "            X_train_embeddings = scaler.fit_transform(X_train_embeddings)\n",
    "            X_val_embeddings = scaler.transform(X_val_embeddings)\n",
    "\n",
    "        # fit model\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train_embeddings, y_train)\n",
    "\n",
    "        train_score = clf.score(X_train_embeddings, y_train)\n",
    "        val_score = clf.score(X_val_embeddings, y_val)\n",
    "        logger.info(f\"score for layer {layer_id} - layer_name {layer_name}, train_score: {train_score:.4f}, val_score: {val_score:.4f}\")\n",
    "        \n",
    "        # store results to csv\n",
    "        row = {}\n",
    "        row['run_id'] = run_id\n",
    "        row['layer_id'] = layer_id\n",
    "        row['layer_name'] = layer_name\n",
    "        row['model'] = model\n",
    "        row['nr_images'] = nr_images\n",
    "        row['normalize'] = normalize\n",
    "        row['train_score'] = train_score\n",
    "        row['val_score'] = val_score\n",
    "        df_row = pd.DataFrame(data=[row])\n",
    "        df_results = pd.concat([df_results, df_row], ignore_index=True, sort=False)\n",
    "        df_results.to_csv(results_csv_path, sep=\";\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if is_file:\n",
    "        # provide arguments from command line\n",
    "        train()\n",
    "    else:\n",
    "        # provide arguments in the code\n",
    "        train(normalize=\"none\", nr_images=81, model=\"mobilenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setdetection",
   "language": "python",
   "name": "setdetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
